{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load diff_coverage.py\n",
    "from __future__ import division\n",
    "from scipy.special import comb\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json\n",
    "from read_data import *\n",
    "import math\n",
    "\n",
    "def Gaussian_Approx(X, mu, std):\n",
    "    return -norm.cdf(x=(X-0.5-mu)/std) + norm.cdf(x=(X+0.5-mu)/std)\n",
    "\n",
    "class Diff_Coverage():\n",
    "\n",
    "    def __init__(self, uniform=True, f=0, flip_p=0.001, flip_q = 0.1, data_src=\"SG\",\n",
    "                 plc_num=200,people=500, candidate_num=50, k_favor=5,\n",
    "                 max_iter=200, stop_iter=200, freeze=False, skew=False):\n",
    "        # constants in rappor\n",
    "        self.f = f\n",
    "        self.p = flip_p\n",
    "        self.q = flip_q\n",
    "\n",
    "        # constants in datasets\n",
    "        self.plc_num = plc_num\n",
    "        self.k_favor = k_favor\n",
    "        self.people = people\n",
    "        self.candidate_num = candidate_num\n",
    "\n",
    "        # vector, different xi for different place if not uniform\n",
    "        self.uniform = uniform\n",
    "        self.xi = None\n",
    "\n",
    "        self.data_source = data_src\n",
    "\n",
    "\n",
    "        # constants in iteration\n",
    "        self.comb_mat = np.load('comb_mat.npy')\n",
    "        self.iter = max_iter\n",
    "        # used in greedy random select\n",
    "        self.stop_iter = stop_iter\n",
    "\n",
    "        # save to disk\n",
    "        self.sample_dir = \"sample.npy\"\n",
    "        self.transfer_dir = \"transfer.npy\"\n",
    "\n",
    "        # paras for regression: a for slope, b for intersect\n",
    "        # vector, different\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "\n",
    "        self._is_train = False\n",
    "        self.transferred_sample = None\n",
    "        self.true_sample = None\n",
    "\n",
    "        self.freeze = freeze\n",
    "        self.prob_mat = None\n",
    "        self.skew = skew\n",
    "\n",
    "    # make table for combination numbers\n",
    "    def make_table(self):\n",
    "        comb_mat = np.zeros((self.candidate_num + 1, self.candidate_num + 1))\n",
    "        for i in range(0, self.candidate_num + 1):\n",
    "            for j in range(0, i + 1):\n",
    "                comb_mat[i][j] = comb(i, j)\n",
    "        np.save('comb_mat', comb_mat)\n",
    "\n",
    "    def update_paras(self):\n",
    "        self.a = np.zeros(self.plc_num)\n",
    "        self.b = np.zeros(self.plc_num)\n",
    "        # default is a uniform prior\n",
    "        self.xi = np.ones(self.plc_num) * self.k_favor / (self.plc_num + 0.0)\n",
    "\n",
    "    def posterior(self, X, plc_id):\n",
    "        N = self.candidate_num\n",
    "        upper_bound = 500\n",
    "        p_0 = self.p - 0.5 * self.f * (self.p - self.q)\n",
    "        p_1 = self.q + 0.5 * self.f * (self.p - self.q)\n",
    "\n",
    "        xi = self.xi[plc_id]\n",
    "        sum_ = 0\n",
    "        numerator = p_0 ** X * (1 - p_0) ** (N - X)\n",
    "        # use Gaussian to approximate Binomial when N is large\n",
    "        if N > upper_bound:\n",
    "            mu = N * p_0\n",
    "            std = math.sqrt(N * p_0 * (1 - p_0))\n",
    "            numerator *= Gaussian_Approx(X,mu,std)\n",
    "        else:\n",
    "            numerator *= self.comb_mat[N, X] * (1 - xi) ** N\n",
    "        for i in range(0, N + 1):\n",
    "            if N > upper_bound:\n",
    "                mu = N * xi\n",
    "                std = math.sqrt(N * xi * (1 - xi))\n",
    "                outer = Gaussian_Approx(i, mu, std)\n",
    "            else:\n",
    "                outer = self.comb_mat[N, i] * xi ** i * (1 - xi) ** (N - i)\n",
    "            if outer == 0.0:\n",
    "                continue\n",
    "            inner = 0\n",
    "            for m in range(max([0, X + i - N]), min([i, X]) + 1):\n",
    "                # sum_ += self.comb_mat[N, i] * self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #     self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m) * \\\n",
    "                #     xi ** i * (1 - xi) ** (N - i)\n",
    "                if i <= upper_bound:\n",
    "                    first_part = self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m)\n",
    "                else:\n",
    "                    mu = i * (1 - p_1)\n",
    "                    std = math.sqrt(i * p_1 * (1 - p_1))\n",
    "                    first_part = Gaussian_Approx(m, mu, std)\n",
    "                if N - i > upper_bound:\n",
    "                    mu = (N-i) * p_0\n",
    "                    std = math.sqrt((N-i) * p_0 * (1 - p_0))\n",
    "                    second_part = Gaussian_Approx(X-m,mu,std)\n",
    "                else:\n",
    "                    second_part = self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                # inner += self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #         self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                inner += first_part * second_part\n",
    "            sum_ += outer * inner\n",
    "        ratio = numerator / sum_\n",
    "        return ratio\n",
    "\n",
    "    # probability distribution for artificial dataset\n",
    "    def prob_dist(self):\n",
    "        plcs = []\n",
    "        y = np.ones(self.plc_num) * 1.0 / self.plc_num\n",
    "\n",
    "        prob_mat = np.zeros((self.plc_num, 3))\n",
    "        prob_mat[:, 0] = np.arange(0, self.plc_num)\n",
    "        prob_mat[:, 1] = y\n",
    "        prob_mat[:, 2] = prob_mat[:, 1].cumsum()\n",
    "\n",
    "        for plc in range(0, self.k_favor):\n",
    "            in_bit = 1\n",
    "            while in_bit == 1:\n",
    "                r = np.random.rand()\n",
    "                plc_num = prob_mat.shape[0]\n",
    "                index = -1\n",
    "                for i in range(0, plc_num - 1):\n",
    "                    if i == 0:\n",
    "                        if prob_mat[i, 2] >= r:\n",
    "                            index = prob_mat[i, 0]\n",
    "                            break\n",
    "                    if prob_mat[i, 2] < r and prob_mat[i + 1, 2] >= r:\n",
    "                        index = prob_mat[i + 1, 0]\n",
    "                        break\n",
    "                if index not in plcs:\n",
    "                    plcs.append(int(index))\n",
    "                    in_bit = 0\n",
    "                # else:\n",
    "                #     print \"Same found\"\n",
    "        return plcs\n",
    "\n",
    "    # read real dataset from different resources\n",
    "    def real_sample(self, draw=False):\n",
    "        if self.data_source == \"MCS\" or \"SG\" or \"CG\":\n",
    "            if self.data_source == \"MCS\":\n",
    "                rd = MCS(k_favor=self.k_favor, people=self.people)\n",
    "                sample = rd.make_grid()\n",
    "                self.plc_num = rd.plc_num\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            elif self.data_source == \"SG\":\n",
    "                rd = SG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            else:\n",
    "                rd = CG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "            if self.skew:\n",
    "                print \"Not real dataset, add artificial groups\"\n",
    "                num = int(self.plc_num/self.k_favor)\n",
    "                full_group = np.zeros((num, self.plc_num))\n",
    "                for row in range(0,num):\n",
    "                    full_group[row, range((row-1)*self.k_favor, row*self.k_favor)] = 1\n",
    "                sample = np.vstack((sample, full_group))\n",
    "                self.people = sample.shape[0]\n",
    "            np.save(self.sample_dir, sample)\n",
    "            print \"Total recruit: \", self.people, \" from places: \", self.plc_num\n",
    "            self.update_paras()\n",
    "            # if not self.uniform:\n",
    "            #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "        else:\n",
    "            sample = np.zeros((self.people, self.plc_num))\n",
    "\n",
    "            # skew: add groups of people covering the whole area\n",
    "            if self.skew:\n",
    "                partition = [0.2, 0.3, 0.5]\n",
    "                self.k_favor = self.plc_num / self.candidate_num\n",
    "                all_plc = list(np.random.permutation(self.plc_num))\n",
    "                for i in range(0, int(self.people * partition[0])):\n",
    "                    mod = i % self.candidate_num\n",
    "                    sample[i,all_plc[mod * self.k_favor: (mod + 1) * self.k_favor]] = 1\n",
    "                same_k_favor = random.sample(range(self.plc_num), self.k_favor * 5)\n",
    "                for i in range(int(self.people * partition[0]), int(self.people * partition[1])):\n",
    "                    small_partition = random.sample(same_k_favor, self.k_favor)\n",
    "                    sample[i, small_partition] = 1\n",
    "                for i in range(int(self.people * partition[1]), int(self.people * partition[2])):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = 1\n",
    "            else:\n",
    "                for i in range(0, self.people):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = np.ones(self.k_favor)\n",
    "            self.update_paras()\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        return sample\n",
    "\n",
    "    def flip(self, bit):\n",
    "        r = np.random.rand()\n",
    "        if r < 0.5 * self.f:\n",
    "            B_prime = 1\n",
    "        elif r < self.f:\n",
    "            B_prime = 0\n",
    "        else:\n",
    "            B_prime = bit\n",
    "        r = np.random.rand()\n",
    "        if B_prime == 1:\n",
    "            if r < self.q:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        else:\n",
    "            if r < self.p:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        return B\n",
    "\n",
    "    # create \"fake\" dataset after random response\n",
    "    def transfer_sample(self, draw=False):\n",
    "        sample = self.real_sample()\n",
    "        self.true_sample = sample.copy()\n",
    "        if not self.uniform:\n",
    "            self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "            self.xi[self.xi == 0] = 1.0 / self.people\n",
    "        for i in range(0, self.people):\n",
    "            r = sample[i, :]\n",
    "            r = np.array(map(lambda bit: self.flip(bit), r))[np.newaxis, :]\n",
    "            sample[i, :] = r\n",
    "\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        self.transferred_sample = sample\n",
    "        np.save(self.transfer_dir, sample)\n",
    "\n",
    "    # sum posterior in terms of locations\n",
    "    def sum_posterior(self, sum_row):\n",
    "        if self.uniform and self.prob_mat is not None:\n",
    "            posterior_per_loc = [self.prob_mat[int(x)] for x in sum_row]\n",
    "        else:\n",
    "            posterior_per_loc = [self.posterior(int(x), id) for id, x in enumerate(sum_row)]\n",
    "        total_sum = np.sum(np.array(posterior_per_loc))\n",
    "        return total_sum\n",
    "\n",
    "    def train(self, use_grad=False, fake=False):\n",
    "        if not os.path.exists(self.sample_dir) or \\\n",
    "                not os.path.exists(self.transfer_dir) or not self.freeze:\n",
    "            self.transfer_sample()\n",
    "            sample = self.transferred_sample\n",
    "        else:\n",
    "            self.transferred_sample = np.load(self.transfer_dir)\n",
    "            self.true_sample = np.load(self.sample_dir)\n",
    "            sample = self.transferred_sample\n",
    "            if not self.uniform and not fake:\n",
    "                self.update_paras()\n",
    "                self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "                # self.xi[np.where(self.xi)==0] = 1 / self.people\n",
    "\n",
    "        # first fit the linear regression coefficients\n",
    "        self.posterior_regression(draw=False)\n",
    "\n",
    "        # choice means those ids which are in the candidate set\n",
    "        choice = random.sample(range(self.people), self.candidate_num)\n",
    "        not_choice = list(set(range(self.people)).difference(set(choice)))\n",
    "\n",
    "        # party means the bit arrays for those candidates\n",
    "        party = sample[choice, :]\n",
    "        unused = sample[not_choice, :]\n",
    "\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print \"Start Training: Original Loss - \" + str(total_sum)\n",
    "\n",
    "        same_count = 0\n",
    "        past_set = set()\n",
    "        for i in range(0, self.iter):\n",
    "\n",
    "            if use_grad:\n",
    "                diff, id_of_r, id_of_r_ = self.grad_boosting(\n",
    "                    sum_row, party, unused)\n",
    "                r = choice[id_of_r]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "                if diff > 0:\n",
    "                    if {r,r_} == past_set:\n",
    "                        same_count += 1\n",
    "                    else:\n",
    "                        past_set = {r, r_}\n",
    "                        same_count = 0\n",
    "                    if same_count >= 3:\n",
    "                        print \"Stop iteration at: \", i\n",
    "                        break\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "\n",
    "                    # print 'Loss reduce by: ', str(diff), 'r and r_ is: ', r, r_\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                id_of_r = np.random.randint(self.candidate_num)\n",
    "                r = choice[id_of_r]\n",
    "                id_of_r_ = random.sample(range(len(unused)), 1)[0]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "\n",
    "                sum_row_new = sum_row - party[id_of_r, :] + sample[r_, :]\n",
    "                total_sum_new = self.sum_posterior(sum_row_new)\n",
    "\n",
    "                if total_sum_new < total_sum:\n",
    "                    total_sum = total_sum_new\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "                    same_count = 0\n",
    "                else:\n",
    "                    same_count += 1\n",
    "\n",
    "                # print str(total_sum), same_count\n",
    "\n",
    "                if same_count > 1 and use_grad:\n",
    "                    break\n",
    "                elif same_count > self.stop_iter:\n",
    "                    break\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print 'Finish:', total_sum, same_count\n",
    "        self._is_train = True\n",
    "        return choice\n",
    "\n",
    "    # compare with two baseline: use \"fake data\" directly and greedy random search\n",
    "    def validate(self, choice, times=10):\n",
    "        if not self._is_train:\n",
    "            exit(1)\n",
    "        # fail = 0\n",
    "        # abnormal = 0\n",
    "        true_sample = self.true_sample\n",
    "        fake_sample = self.transferred_sample\n",
    "\n",
    "        result = []\n",
    "        true_opt_party = true_sample[choice, :]\n",
    "        fake_opt_party = fake_sample[choice, :]\n",
    "        sum_row = np.sum(fake_opt_party, axis=0)\n",
    "        total_sum_opt = self.sum_posterior(sum_row)\n",
    "        coverage_opt = np.sum(np.sum(true_opt_party, axis=0) > 0)\n",
    "        result.append((coverage_opt, total_sum_opt))\n",
    "        for i in range(times):\n",
    "            # totally random pick:\n",
    "            print \"Random Pick\"\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            result.append((coverage_rand, total_sum_rand))\n",
    "\n",
    "            # greedy search\n",
    "            # print \"Rand_Greedy\"\n",
    "            # rand_greedy_choice = self.train(use_grad=False)\n",
    "            # rand_greedy_party = true_sample[rand_greedy_choice,:]\n",
    "            # rand_greedy_fake_party = fake_sample[rand_greedy_choice,:]\n",
    "            #\n",
    "            # sum_row = np.sum(rand_greedy_fake_party, axis=0)\n",
    "            # total_sum_rand_greedy = self.sum_posterior(sum_row)\n",
    "            # coverage_rand_greedy = np.sum(np.sum(rand_greedy_party, axis=0) > 0)\n",
    "            # result.append((coverage_rand_greedy, total_sum_rand_greedy))\n",
    "\n",
    "            # take fake as real\n",
    "\n",
    "            print \"Noisy\"\n",
    "            raw_p, raw_q = (self.p, self.q)\n",
    "            self.p,self.q = (1e-10, 1-1e-10)\n",
    "            if not self.uniform:\n",
    "                raw_xi = self.xi\n",
    "                self.xi = np.sum(fake_sample, axis=0) / self.people\n",
    "            fake_choice = self.train(use_grad=True,fake=True)\n",
    "            # print fake_choice\n",
    "            fake_party = true_sample[fake_choice,:]\n",
    "            fake_fake_party = fake_sample[fake_choice,:]\n",
    "\n",
    "            sum_row = np.sum(fake_fake_party, axis=0)\n",
    "            total_sum_fake = self.sum_posterior(sum_row)\n",
    "            coverage_fake = np.sum(np.sum(fake_party, axis=0) > 0)\n",
    "            result.append((coverage_fake, total_sum_fake))\n",
    "            self.p, self.q = (raw_p, raw_q)\n",
    "            if not self.uniform:\n",
    "                self.xi = raw_xi\n",
    "\n",
    "        # first element is opt, second is random-greedy, third is fake-data\n",
    "        return result\n",
    "\n",
    "    # plot random select results to find relationship between loss function and target\n",
    "    def random_select(self, random_choice=50, file_name='p_0.01'):\n",
    "        true_sample = np.load(self.sample_dir)\n",
    "        fake_sample = np.load(self.transfer_dir)\n",
    "        rand_prob_ = np.zeros(random_choice)\n",
    "        rand_region_ = np.zeros(random_choice)\n",
    "\n",
    "        for i in range(0, random_choice):\n",
    "            self.iter = np.random.randint(1000, 1500)\n",
    "            self.stop_iter = np.random.randint(50, 100)\n",
    "\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            # choice = self.train()\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            rand_prob_[i] = total_sum_rand\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            rand_region_[i] = coverage_rand\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print i\n",
    "\n",
    "        # plt.scatter(real_prob_, real_region,c='r')\n",
    "        plt.scatter(rand_prob_, rand_region_, c='b')\n",
    "        plt.xlabel(\"Sum of posterior over location\")\n",
    "        plt.ylabel(\"Coverage\")\n",
    "        title = \"transfer_prob_=\" + str(self.p) + \", candidate_num=\" + str(\n",
    "            self.candidate_num) + \", k_favor=\" + str(self.k_favor)\n",
    "        plt.title(title)\n",
    "        plt.savefig('img/' + file_name + '.png')\n",
    "        print self.p, self.k_favor, self.candidate_num\n",
    "\n",
    "    def posterior_regression(self, draw=False):\n",
    "        # now coefficients should be a matrix, each place has its own slope and intercept\n",
    "\n",
    "        # only take several points to fit the linear regression,\n",
    "        # avoiding high cost of iteration\n",
    "\n",
    "        # x_max = self.candidate_num + 1\n",
    "        x_max = 10\n",
    "        def find_coeffs(plc_id):\n",
    "            X_range = np.arange(0, x_max)\n",
    "            stop_count = self.candidate_num + 1\n",
    "            ans = np.zeros(X_range.shape)\n",
    "            for X in X_range:\n",
    "                ans[X] = np.log(self.posterior(X=X, plc_id=plc_id))\n",
    "                if ans[X] == -np.inf or ans[X] == np.nan:\n",
    "                    ans[X] = 0\n",
    "                    stop_count = X\n",
    "                    break\n",
    "            X_range = X_range[0:stop_count, np.newaxis]\n",
    "            ans = ans[0:stop_count, np.newaxis]\n",
    "            if (draw):\n",
    "                plt.scatter(X_range, ans)\n",
    "                plt.show()\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(X_range, ans)\n",
    "            return lr.coef_, lr.intercept_\n",
    "\n",
    "        if self.uniform:\n",
    "            # print \"Make posterior tables!\"\n",
    "            self.prob_mat = np.array([self.posterior(x,1) for x in range(self.candidate_num + 1)])\n",
    "            self.prob_mat[np.isnan(self.prob_mat)] = 0\n",
    "            coeffs = find_coeffs(plc_id=0)\n",
    "            self.a = np.ones(self.a.shape) * coeffs[0]\n",
    "            self.b = np.ones(self.b.shape) * coeffs[1]\n",
    "        else:\n",
    "            # self.prob_mat = np.zeros((self.plc_num, self.candidate_num + 1))\n",
    "            # plc with same prior just need one time calculation\n",
    "            prior_dict = {}\n",
    "\n",
    "            for plc_id in range(self.plc_num):\n",
    "                xi_id = int(np.around(self.xi[plc_id] * self.people))\n",
    "                if prior_dict.has_key(xi_id):\n",
    "                    # self.prob_mat[plc_id,:] = self.prob_mat[prior_dict[xi_id],:]\n",
    "                    self.a[plc_id] = self.a[prior_dict[xi_id]]\n",
    "                    self.b[plc_id] = self.b[prior_dict[xi_id]]\n",
    "                else:\n",
    "                    # for x in range(0,self.candidate_num+1):\n",
    "                    #     self.prob_mat[plc_id, x] = self.posterior(X=x, plc_id=plc_id)\n",
    "                    prior_dict[xi_id] = plc_id\n",
    "                    coeffs = find_coeffs(plc_id)\n",
    "                    self.a[plc_id] = coeffs[0]\n",
    "                    self.b[plc_id] = coeffs[1]\n",
    "                # if xi_id == 0:\n",
    "                #     print self.a[plc_id], self.b[plc_id]\n",
    "            print \"Total different prior: \", len(prior_dict)\n",
    "\n",
    "        print \"Linear Regression over\"\n",
    "\n",
    "    def grad_boosting(self, sum_row, party, unused):\n",
    "\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_sub = np.dot(party, grad_)\n",
    "        id_of_r = np.argmin(candidate_sub)\n",
    "\n",
    "        # re-calculate grad_ using party after removing a candidate\n",
    "        party[id_of_r,:] = 0\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_add = np.dot(unused, grad_)\n",
    "\n",
    "        # return their sequential id, not original id in sample\n",
    "        id_of_r_ = np.argmax(candidate_add)\n",
    "\n",
    "        diff = np.max(candidate_add) - np.min(candidate_sub)\n",
    "        return diff, int(id_of_r), int(id_of_r_)\n",
    "\n",
    "def simulate_pipeline(change_k=False, change_p=False, change_cand=False, add_skew=False):\n",
    "    if change_k:\n",
    "        changed_paras = range(10,40,10)\n",
    "        dir_name = 'change_k'\n",
    "    elif change_p:\n",
    "        changed_paras = [0.1, 0.01, 0.001]\n",
    "        dir_name = 'change_p'\n",
    "    elif change_cand:\n",
    "        changed_paras = range(40,45,2)\n",
    "        dir_name = 'change_cand'\n",
    "    elif add_skew:\n",
    "        changed_paras = [True, False]\n",
    "        dir_name = 'add_skew'\n",
    "    else:\n",
    "        dir_name = 'test'\n",
    "        changed_paras = [0]\n",
    "\n",
    "    try:\n",
    "        os.listdir(dir_name)\n",
    "    except OSError:\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    dataset_num = len(changed_paras)\n",
    "    iter_per_set = 10\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        print \"Start a new set!\"\n",
    "        plc_num = 1000\n",
    "        people = 600\n",
    "        candidate = 40\n",
    "        p = 1e-3\n",
    "        q = 1-p\n",
    "        k_favor = 25\n",
    "        skew = True\n",
    "\n",
    "        if change_k:\n",
    "            k_favor = changed_paras[i]\n",
    "        elif change_cand:\n",
    "            candidate = changed_paras[i]\n",
    "        elif change_p:\n",
    "            p = changed_paras[i]\n",
    "        elif add_skew:\n",
    "            skew = changed_paras[i]\n",
    "\n",
    "        new_simulation = Diff_Coverage(flip_p=p, flip_q=q, candidate_num=candidate,\n",
    "                                       plc_num=plc_num, people=people, k_favor=k_favor, max_iter=4000,\n",
    "                                       data_src=\"SG\", uniform=False,freeze=True,skew=skew)\n",
    "        try:\n",
    "            os.remove(new_simulation.sample_dir)\n",
    "        except:\n",
    "            print \"No real sample detected!\"\n",
    "        try:\n",
    "            os.remove(new_simulation.transfer_dir)\n",
    "        except:\n",
    "            print \"No transferred data detected!\"\n",
    "        # new_simulation.real_sample(draw=True, skew=True, real=False)\n",
    "        # new_simulation.transfer_sample(draw=False)\n",
    "        # new_simulation.posterior_regression(draw=True)\n",
    "\n",
    "        for j in range(0, iter_per_set):\n",
    "            choice = new_simulation.train(use_grad=True)\n",
    "            result = new_simulation.validate(choice, times=1)\n",
    "            print result\n",
    "            with open(dir_name+ '/' + new_simulation.data_source + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\", 'w') as f:\n",
    "                f.write(json.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5855053951\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxcFhZmZZcXCYmVlW/j/oCJ9BQyHjMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1114f4bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def easy_draw():\n",
    "    dataset_num = 1\n",
    "    method_num = 3\n",
    "    iter_per_set = 10\n",
    "    dir_name = \"test\"\n",
    "    data_src = \"SG\"\n",
    "    color = ['c','r','b','g']\n",
    "    Results = np.zeros((dataset_num, method_num, iter_per_set))\n",
    "    Stats = np.zeros((dataset_num, method_num, 2))#0 for mean, 1 for std\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        for j in range(0, iter_per_set):\n",
    "            with open(dir_name+ '/' + data_src + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\",'r') as f:\n",
    "                results = json.loads(f.read())\n",
    "                for m in range(0, method_num):\n",
    "                    Results[i,m,j] = results[m][0]\n",
    "                    \n",
    "    for i in range(0,dataset_num):\n",
    "        for m in range(0, method_num):\n",
    "            Stats[i,m,0] = np.mean(Results[i,m,:])\n",
    "            Stats[i,m,1] = np.std(Results[i,m,:])\n",
    "\n",
    "    print Stats[0,0,1]\n",
    "    ind = np.arange(dataset_num)\n",
    "    width = 0.2\n",
    "    p_list = []\n",
    "    for m in range(0, method_num):\n",
    "        if method_num % 2 != 0:\n",
    "            bias = np.floor(method_num/2)\n",
    "        else:\n",
    "            bias = (method_num - 1.0)/2\n",
    "        p = plt.bar(ind + width*(m-bias), Stats[:,m,0], width=width, color=color[m], yerr=Stats[:,m,1])\n",
    "        p_list.append(p)\n",
    "            \n",
    "#     marker = ['p_0.1','p_0.01','p_0.001']\n",
    "#     marker = ['cand=30','cand=60','cand=90','cand=120']\n",
    "#     marker = ['k=10','k=20','k=30']\n",
    "    marker = ['cand=40','cand=42','cand=44']\n",
    "    plt.xticks(ind, marker)\n",
    "    plt.ylim(0,1000)\n",
    "    tup = tuple([p[0] for p in p_list])\n",
    "    plt.legend(tup,(\"Greedy\", \"Random\",\"Noisy\"))\n",
    "    plt.ylabel(\"Num of Covered Targets\")\n",
    "    plt.savefig(data_src+'_' + dir_name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "easy_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n",
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCjE4JEmF/H/NdPB4LgYX2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bcb1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression over\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py27",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
