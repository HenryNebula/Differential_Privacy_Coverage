{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 5, 3, 2]), array([ 8,  9,  9, 10, 10]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([9, 8, 10, 10, 3, 9, 0, 4, 6, 0])\n",
    "ind = np.argpartition(a, -5)[-5:]\n",
    "ind, a[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = a.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([149, 100, 154, 196, 171, 164,  33]),\n",
       " array([ 0.98145186,  0.98168283,  0.98800988,  0.98870129,  0.98738235,\n",
       "         0.9977841 ,  0.99363137]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=7\n",
    "grad_ = np.random.rand(200,)\n",
    "ind = np.argpartition(grad_, -k)[-k:]\n",
    "ind, grad_[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a[[1,2,3]] = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     9, 100000, 100000, 100000,      3,      9,      0,      4,\n",
       "            6,      0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load diff_coverage.py\n",
    "from __future__ import division\n",
    "from scipy.special import comb\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json\n",
    "from read_data import *\n",
    "import math\n",
    "\n",
    "def Gaussian_Approx(X, mu, std):\n",
    "    return -norm.cdf(x=(X-0.5-mu)/std) + norm.cdf(x=(X+0.5-mu)/std)\n",
    "\n",
    "class Diff_Coverage():\n",
    "\n",
    "    def __init__(self, uniform=True, f=0, flip_p=0.001, flip_q = 0.1, data_src=\"SG\",\n",
    "                 plc_num=200,people=500, candidate_num=50, k_favor=5,\n",
    "                 max_iter=200, stop_iter=200, freeze=False, skew=False):\n",
    "        # constants in rappor\n",
    "        self.f = f\n",
    "        self.p = flip_p\n",
    "        self.q = flip_q\n",
    "\n",
    "        # constants in datasets\n",
    "        self.plc_num = plc_num\n",
    "        self.k_favor = k_favor\n",
    "        self.people = people\n",
    "        self.candidate_num = candidate_num\n",
    "\n",
    "        # vector, different xi for different place if not uniform\n",
    "        self.uniform = uniform\n",
    "        self.xi = None\n",
    "\n",
    "        self.data_source = data_src\n",
    "\n",
    "\n",
    "        # constants in iteration\n",
    "        self.comb_mat = np.load('comb_mat.npy')\n",
    "        self.iter = max_iter\n",
    "        # used in greedy random select\n",
    "        self.stop_iter = stop_iter\n",
    "\n",
    "        # save to disk\n",
    "        self.sample_dir = \"sample.npy\"\n",
    "        self.transfer_dir = \"transfer.npy\"\n",
    "\n",
    "        # paras for regression: a for slope, b for intersect\n",
    "        # vector, different\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "\n",
    "        self._is_train = False\n",
    "        self.transferred_sample = None\n",
    "        self.true_sample = None\n",
    "\n",
    "        self.freeze = freeze\n",
    "        self.prob_mat = None\n",
    "        self.skew = skew\n",
    "\n",
    "    # make table for combination numbers\n",
    "    def make_table(self):\n",
    "        comb_mat = np.zeros((self.candidate_num + 1, self.candidate_num + 1))\n",
    "        for i in range(0, self.candidate_num + 1):\n",
    "            for j in range(0, i + 1):\n",
    "                comb_mat[i][j] = comb(i, j)\n",
    "        np.save('comb_mat', comb_mat)\n",
    "\n",
    "    def update_paras(self):\n",
    "        self.a = np.zeros(self.plc_num)\n",
    "        self.b = np.zeros(self.plc_num)\n",
    "        # default is a uniform prior\n",
    "        self.xi = np.ones(self.plc_num) * self.k_favor / (self.plc_num + 0.0)\n",
    "\n",
    "    def posterior(self, X, plc_id):\n",
    "        N = self.candidate_num\n",
    "        upper_bound = 500\n",
    "        p_0 = self.p - 0.5 * self.f * (self.p - self.q)\n",
    "        p_1 = self.q + 0.5 * self.f * (self.p - self.q)\n",
    "\n",
    "        xi = self.xi[plc_id]\n",
    "        sum_ = 0\n",
    "        numerator = p_0 ** X * (1 - p_0) ** (N - X)\n",
    "        # use Gaussian to approximate Binomial when N is large\n",
    "        if N > upper_bound:\n",
    "            mu = N * p_0\n",
    "            std = math.sqrt(N * p_0 * (1 - p_0))\n",
    "            numerator *= Gaussian_Approx(X,mu,std)\n",
    "        else:\n",
    "            numerator *= self.comb_mat[N, X] * (1 - xi) ** N\n",
    "        for i in range(0, N + 1):\n",
    "            if N > upper_bound:\n",
    "                mu = N * xi\n",
    "                std = math.sqrt(N * xi * (1 - xi))\n",
    "                outer = Gaussian_Approx(i, mu, std)\n",
    "            else:\n",
    "                outer = self.comb_mat[N, i] * xi ** i * (1 - xi) ** (N - i)\n",
    "            if outer == 0.0:\n",
    "                continue\n",
    "            inner = 0\n",
    "            for m in range(max([0, X + i - N]), min([i, X]) + 1):\n",
    "                # sum_ += self.comb_mat[N, i] * self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #     self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m) * \\\n",
    "                #     xi ** i * (1 - xi) ** (N - i)\n",
    "                if i <= upper_bound:\n",
    "                    first_part = self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m)\n",
    "                else:\n",
    "                    mu = i * (1 - p_1)\n",
    "                    std = math.sqrt(i * p_1 * (1 - p_1))\n",
    "                    first_part = Gaussian_Approx(m, mu, std)\n",
    "                if N - i > upper_bound:\n",
    "                    mu = (N-i) * p_0\n",
    "                    std = math.sqrt((N-i) * p_0 * (1 - p_0))\n",
    "                    second_part = Gaussian_Approx(X-m,mu,std)\n",
    "                else:\n",
    "                    second_part = self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                # inner += self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #         self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                inner += first_part * second_part\n",
    "            sum_ += outer * inner\n",
    "        ratio = numerator / sum_\n",
    "        return ratio\n",
    "\n",
    "    # probability distribution for artificial dataset\n",
    "    def prob_dist(self):\n",
    "        plcs = []\n",
    "        y = np.ones(self.plc_num) * 1.0 / self.plc_num\n",
    "\n",
    "        prob_mat = np.zeros((self.plc_num, 3))\n",
    "        prob_mat[:, 0] = np.arange(0, self.plc_num)\n",
    "        prob_mat[:, 1] = y\n",
    "        prob_mat[:, 2] = prob_mat[:, 1].cumsum()\n",
    "\n",
    "        for plc in range(0, self.k_favor):\n",
    "            in_bit = 1\n",
    "            while in_bit == 1:\n",
    "                r = np.random.rand()\n",
    "                plc_num = prob_mat.shape[0]\n",
    "                index = -1\n",
    "                for i in range(0, plc_num - 1):\n",
    "                    if i == 0:\n",
    "                        if prob_mat[i, 2] >= r:\n",
    "                            index = prob_mat[i, 0]\n",
    "                            break\n",
    "                    if prob_mat[i, 2] < r and prob_mat[i + 1, 2] >= r:\n",
    "                        index = prob_mat[i + 1, 0]\n",
    "                        break\n",
    "                if index not in plcs:\n",
    "                    plcs.append(int(index))\n",
    "                    in_bit = 0\n",
    "                # else:\n",
    "                #     print \"Same found\"\n",
    "        return plcs\n",
    "\n",
    "    # read real dataset from different resources\n",
    "    def real_sample(self, draw=False):\n",
    "        if self.data_source == \"MCS\" or \"SG\" or \"CG\":\n",
    "            if self.data_source == \"MCS\":\n",
    "                rd = MCS(k_favor=self.k_favor, people=self.people)\n",
    "                sample = rd.make_grid()\n",
    "                self.plc_num = rd.plc_num\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            elif self.data_source == \"SG\":\n",
    "                rd = SG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            else:\n",
    "                rd = CG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "            if self.skew:\n",
    "                print \"Not real dataset, add artificial groups\"\n",
    "                num = int(self.plc_num/self.k_favor)\n",
    "                full_group = np.zeros((num, self.plc_num))\n",
    "                for row in range(0,num):\n",
    "                    full_group[row, range((row-1)*self.k_favor, row*self.k_favor)] = 1\n",
    "                sample = np.vstack((sample, full_group))\n",
    "                self.people = sample.shape[0]\n",
    "            np.save(self.sample_dir, sample)\n",
    "            print \"Total recruit: \", self.people, \" from places: \", self.plc_num\n",
    "            self.update_paras()\n",
    "            # if not self.uniform:\n",
    "            #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "        else:\n",
    "            sample = np.zeros((self.people, self.plc_num))\n",
    "\n",
    "            # skew: add groups of people covering the whole area\n",
    "            if self.skew:\n",
    "                partition = [0.2, 0.3, 0.5]\n",
    "                self.k_favor = self.plc_num / self.candidate_num\n",
    "                all_plc = list(np.random.permutation(self.plc_num))\n",
    "                for i in range(0, int(self.people * partition[0])):\n",
    "                    mod = i % self.candidate_num\n",
    "                    sample[i,all_plc[mod * self.k_favor: (mod + 1) * self.k_favor]] = 1\n",
    "                same_k_favor = random.sample(range(self.plc_num), self.k_favor * 5)\n",
    "                for i in range(int(self.people * partition[0]), int(self.people * partition[1])):\n",
    "                    small_partition = random.sample(same_k_favor, self.k_favor)\n",
    "                    sample[i, small_partition] = 1\n",
    "                for i in range(int(self.people * partition[1]), int(self.people * partition[2])):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = 1\n",
    "            else:\n",
    "                for i in range(0, self.people):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = np.ones(self.k_favor)\n",
    "            self.update_paras()\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        return sample\n",
    "\n",
    "    def flip(self, bit):\n",
    "        r = np.random.rand()\n",
    "        if r < 0.5 * self.f:\n",
    "            B_prime = 1\n",
    "        elif r < self.f:\n",
    "            B_prime = 0\n",
    "        else:\n",
    "            B_prime = bit\n",
    "        r = np.random.rand()\n",
    "        if B_prime == 1:\n",
    "            if r < self.q:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        else:\n",
    "            if r < self.p:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        return B\n",
    "\n",
    "    # create \"fake\" dataset after random response\n",
    "    def transfer_sample(self, draw=False):\n",
    "        sample = self.real_sample()\n",
    "        self.true_sample = sample.copy()\n",
    "        if not self.uniform:\n",
    "            self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "        for i in range(0, self.people):\n",
    "            r = sample[i, :]\n",
    "            r = np.array(map(lambda bit: self.flip(bit), r))[np.newaxis, :]\n",
    "            sample[i, :] = r\n",
    "\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        self.transferred_sample = sample\n",
    "        np.save(self.transfer_dir, sample)\n",
    "\n",
    "    # sum posterior in terms of locations\n",
    "    def sum_posterior(self, sum_row):\n",
    "        if self.uniform and self.prob_mat is not None:\n",
    "            posterior_per_loc = [self.prob_mat[int(x)] for x in sum_row]\n",
    "        else:\n",
    "            posterior_per_loc = [self.posterior(int(x), id) for id, x in enumerate(sum_row)]\n",
    "        total_sum = np.sum(np.array(posterior_per_loc))\n",
    "        return total_sum\n",
    "\n",
    "    def train(self, use_grad=False):\n",
    "        if not os.path.exists(self.sample_dir) or \\\n",
    "                not os.path.exists(self.transfer_dir) or not self.freeze:\n",
    "            self.transfer_sample()\n",
    "            sample = self.transferred_sample\n",
    "        else:\n",
    "            self.transferred_sample = np.load(self.transfer_dir)\n",
    "            self.true_sample = np.load(self.sample_dir)\n",
    "            sample = self.transferred_sample\n",
    "            self.update_paras()\n",
    "            if not self.uniform:\n",
    "                self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "\n",
    "        # first fit the linear regression coefficients\n",
    "        self.posterior_regression(draw=False)\n",
    "\n",
    "        # choice means those ids which are in the candidate set\n",
    "        choice = random.sample(range(self.people), self.candidate_num)\n",
    "        not_choice = list(set(range(self.people)).difference(set(choice)))\n",
    "\n",
    "        # party means the bit arrays for those candidates\n",
    "        party = sample[choice, :]\n",
    "        unused = sample[not_choice, :]\n",
    "\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print \"Start Training: Original Loss - \" + str(total_sum)\n",
    "\n",
    "        same_count = 0\n",
    "        past_set = set()\n",
    "        for i in range(0, self.iter):\n",
    "\n",
    "            if use_grad:\n",
    "                diff, id_of_r, id_of_r_ = self.grad_boosting(\n",
    "                    sum_row, party, unused)\n",
    "                r = choice[id_of_r]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "                if diff > 0:\n",
    "                    if {r,r_} == past_set:\n",
    "                        same_count += 1\n",
    "                    else:\n",
    "                        past_set = {r, r_}\n",
    "                        same_count = 0\n",
    "                    if same_count >= 3:\n",
    "                        print \"Stop iteration at: \", i\n",
    "                        break\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "\n",
    "                    # print 'Loss reduce by: ', str(diff), 'r and r_ is: ', r, r_\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                id_of_r = np.random.randint(self.candidate_num)\n",
    "                r = choice[id_of_r]\n",
    "                id_of_r_ = random.sample(range(len(unused)), 1)[0]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "\n",
    "                sum_row_new = sum_row - party[id_of_r, :] + sample[r_, :]\n",
    "                total_sum_new = self.sum_posterior(sum_row_new)\n",
    "\n",
    "                if total_sum_new < total_sum:\n",
    "                    total_sum = total_sum_new\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "                    same_count = 0\n",
    "                else:\n",
    "                    same_count += 1\n",
    "\n",
    "                # print str(total_sum), same_count\n",
    "\n",
    "                if same_count > 1 and use_grad:\n",
    "                    break\n",
    "                elif same_count > self.stop_iter:\n",
    "                    break\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print 'Finish:', total_sum, same_count\n",
    "        self._is_train = True\n",
    "        return choice\n",
    "\n",
    "    # compare with two baseline: use \"fake data\" directly and greedy random search\n",
    "    def validate(self, choice, times=10):\n",
    "        if not self._is_train:\n",
    "            exit(1)\n",
    "        # fail = 0\n",
    "        # abnormal = 0\n",
    "        true_sample = self.true_sample\n",
    "        fake_sample = self.transferred_sample\n",
    "\n",
    "        result = []\n",
    "        true_opt_party = true_sample[choice, :]\n",
    "        fake_opt_party = fake_sample[choice, :]\n",
    "        sum_row = np.sum(fake_opt_party, axis=0)\n",
    "        total_sum_opt = self.sum_posterior(sum_row)\n",
    "        coverage_opt = np.sum(np.sum(true_opt_party, axis=0) > 0)\n",
    "        result.append((coverage_opt, total_sum_opt))\n",
    "        for i in range(times):\n",
    "            # totally random pick:\n",
    "            print \"Random Pick\"\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            result.append((coverage_rand, total_sum_rand))\n",
    "\n",
    "            # greedy search\n",
    "            # print \"Rand_Greedy\"\n",
    "            # rand_greedy_choice = self.train(use_grad=False)\n",
    "            # rand_greedy_party = true_sample[rand_greedy_choice,:]\n",
    "            # rand_greedy_fake_party = fake_sample[rand_greedy_choice,:]\n",
    "            #\n",
    "            # sum_row = np.sum(rand_greedy_fake_party, axis=0)\n",
    "            # total_sum_rand_greedy = self.sum_posterior(sum_row)\n",
    "            # coverage_rand_greedy = np.sum(np.sum(rand_greedy_party, axis=0) > 0)\n",
    "            # result.append((coverage_rand_greedy, total_sum_rand_greedy))\n",
    "\n",
    "            # take fake as real\n",
    "\n",
    "            print \"Fake as Real\"\n",
    "\n",
    "            raw_p, raw_q = (self.p, self.q)\n",
    "            self.p,self.q = (1e-10, 1-1e-10)\n",
    "            if not self.uniform:\n",
    "                raw_xi = self.xi\n",
    "                self.xi = np.sum(fake_sample, axis=0) / self.people\n",
    "            fake_choice = self.train(use_grad=True)\n",
    "            fake_party = true_sample[fake_choice,:]\n",
    "            fake_fake_party = fake_sample[fake_choice,:]\n",
    "\n",
    "            sum_row = np.sum(fake_fake_party, axis=0)\n",
    "            total_sum_fake = self.sum_posterior(sum_row)\n",
    "            coverage_fake = np.sum(np.sum(fake_party, axis=0) > 0)\n",
    "            result.append((coverage_fake, total_sum_fake))\n",
    "            self.p, self.q = (raw_p, raw_q)\n",
    "            if not self.uniform:\n",
    "                self.xi = raw_xi\n",
    "\n",
    "            # print \"posterior probability: \", total_sum_rand\n",
    "            # print \"random select: \", coverage_rand\n",
    "\n",
    "            # print \"posterior probability: \", total_sum_opt\n",
    "            # print \"true select: \", coverage_opt\n",
    "            # print \"*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\"\n",
    "\n",
    "            # if coverage_rand > coverage_opt:\n",
    "            #     fail += 1\n",
    "            #     if total_sum_rand > total_sum_opt:\n",
    "            #         abnormal += 1\n",
    "\n",
    "        # print \"abnormal rate: \" + str(abnormal) + '/' + str(times)\n",
    "        # print \"fail rate: \" + str(fail) + '/' + str(times)\n",
    "\n",
    "        # first element is opt, second is random-greedy, third is fake-data\n",
    "        return result\n",
    "\n",
    "    # plot random select results to find relationship between loss function and target\n",
    "    def random_select(self, random_choice=50, file_name='p_0.01'):\n",
    "        true_sample = np.load(self.sample_dir)\n",
    "        fake_sample = np.load(self.transfer_dir)\n",
    "        rand_prob_ = np.zeros(random_choice)\n",
    "        rand_region_ = np.zeros(random_choice)\n",
    "\n",
    "        for i in range(0, random_choice):\n",
    "            self.iter = np.random.randint(1000, 1500)\n",
    "            self.stop_iter = np.random.randint(50, 100)\n",
    "\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            # choice = self.train()\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            rand_prob_[i] = total_sum_rand\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            rand_region_[i] = coverage_rand\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print i\n",
    "\n",
    "        # plt.scatter(real_prob_, real_region,c='r')\n",
    "        plt.scatter(rand_prob_, rand_region_, c='b')\n",
    "        plt.xlabel(\"Sum of posterior over location\")\n",
    "        plt.ylabel(\"Coverage\")\n",
    "        title = \"transfer_prob_=\" + str(self.p) + \", candidate_num=\" + str(\n",
    "            self.candidate_num) + \", k_favor=\" + str(self.k_favor)\n",
    "        plt.title(title)\n",
    "        plt.savefig('img/' + file_name + '.png')\n",
    "        print self.p, self.k_favor, self.candidate_num\n",
    "\n",
    "    def posterior_regression(self, draw=False):\n",
    "        # now coefficients should be a matrix, each place has its own slope and intercept\n",
    "\n",
    "        # only take several points to fit the linear regression,\n",
    "        # avoiding high cost of iteration\n",
    "\n",
    "        # x_max = self.candidate_num + 1\n",
    "        x_max = 10\n",
    "        def find_coeffs(plc_id):\n",
    "            X_range = np.arange(0, x_max)\n",
    "            stop_count = self.candidate_num + 1\n",
    "            ans = np.zeros(X_range.shape)\n",
    "            for X in X_range:\n",
    "                ans[X] = np.log(self.posterior(X=X, plc_id=plc_id))\n",
    "                if ans[X] == -np.inf or ans[X] == np.nan:\n",
    "                    ans[X] = 0\n",
    "                    stop_count = X\n",
    "                    break\n",
    "            X_range = X_range[0:stop_count, np.newaxis]\n",
    "            ans = ans[0:stop_count, np.newaxis]\n",
    "            if (draw):\n",
    "                plt.scatter(X_range, ans)\n",
    "                plt.show()\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(X_range, ans)\n",
    "            return lr.coef_, lr.intercept_\n",
    "\n",
    "        if self.uniform:\n",
    "            # print \"Make posterior tables!\"\n",
    "            self.prob_mat = np.array([self.posterior(x,1) for x in range(self.candidate_num + 1)])\n",
    "            self.prob_mat[np.isnan(self.prob_mat)] = 0\n",
    "            coeffs = find_coeffs(plc_id=0)\n",
    "            self.a = np.ones(self.a.shape) * coeffs[0]\n",
    "            self.b = np.ones(self.b.shape) * coeffs[1]\n",
    "        else:\n",
    "            self.prob_mat = np.zeros((self.plc_num, self.candidate_num + 1))\n",
    "            # plc with same prior just need one time calculation\n",
    "            prior_dict = {}\n",
    "\n",
    "            for plc_id in range(self.plc_num):\n",
    "                xi_id = int(np.around(self.xi[plc_id] * self.people))\n",
    "                if prior_dict.has_key(xi_id):\n",
    "                    # self.prob_mat[plc_id,:] = self.prob_mat[prior_dict[xi_id],:]\n",
    "                    self.a[plc_id] = self.a[prior_dict[xi_id]]\n",
    "                    self.b[plc_id] = self.b[prior_dict[xi_id]]\n",
    "                else:\n",
    "                    # for x in range(0,self.candidate_num+1):\n",
    "                    #     self.prob_mat[plc_id, x] = self.posterior(X=x, plc_id=plc_id)\n",
    "                    prior_dict[xi_id] = plc_id\n",
    "                    coeffs = find_coeffs(plc_id)\n",
    "                    self.a[plc_id] = coeffs[0]\n",
    "                    self.b[plc_id] = coeffs[1]\n",
    "\n",
    "            print \"Total different prior: \", len(prior_dict)\n",
    "        print \"Linear Regression over\"\n",
    "\n",
    "    def grad_boosting(self, sum_row, party, unused):\n",
    "\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_sub = np.dot(party, grad_)\n",
    "        id_of_r = np.argmin(candidate_sub)\n",
    "\n",
    "        # re-calculate grad_ using party after removing a candidate\n",
    "        party[id_of_r,:] = 0\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_add = np.dot(unused, grad_)\n",
    "\n",
    "        # return their sequential id, not original id in sample\n",
    "        id_of_r_ = np.argmax(candidate_add)\n",
    "\n",
    "        diff = np.max(candidate_add) - np.min(candidate_sub)\n",
    "        return diff, int(id_of_r), int(id_of_r_)\n",
    "\n",
    "\n",
    "# tool for examining simulation results\n",
    "def stat_compute():\n",
    "    dataset_num = 20\n",
    "    iter_per_set = 20\n",
    "    real = 'real_result/'\n",
    "    simu = 'simulation/'\n",
    "    dir_ = simu\n",
    "    true_means = np.zeros((dataset_num,))\n",
    "    true_stds = np.zeros((dataset_num,))\n",
    "    rand_means = np.zeros((dataset_num,))\n",
    "    rand_stds = np.zeros((dataset_num,))\n",
    "    file_list = ['p_0.1/','p_0.01/','p_0.001/']\n",
    "    whole_mean = np.zeros((len(file_list),2))\n",
    "    whole_std = np.zeros((len(file_list),2))\n",
    "    for count, fn in enumerate(file_list):\n",
    "        whole_true = []\n",
    "        whole_rand = []\n",
    "        for i in range(0, dataset_num):\n",
    "            true = []\n",
    "            rand = []\n",
    "            for j in range(0, iter_per_set):\n",
    "                with open(dir_ + fn + str(i) + '_' + str(j) + \".json\",'r') as f:\n",
    "                    results = json.loads(f.read())\n",
    "                    true.append(results[0][0])\n",
    "                    whole_true.append((results[0]))\n",
    "                    rand += [i[0] for i in results[1:]]\n",
    "                    whole_rand += [i[0] for i in results[1:]]\n",
    "            true_means[i] = np.mean(np.array(true))\n",
    "            true_stds[i] = np.std(np.array(true))\n",
    "            rand_means[i] = np.mean(np.array(rand))\n",
    "            rand_stds[i] = np.std(np.array(rand))\n",
    "\n",
    "        whole_mean[count, 0] = np.mean(np.array(whole_true))\n",
    "        whole_std[count, 0] = np.std(np.array(whole_true))\n",
    "        whole_mean[count, 1] = np.mean(np.array(whole_rand))\n",
    "        whole_std[count, 1] = np.std(np.array(whole_rand))\n",
    "\n",
    "    ind = np.arange(len(file_list))\n",
    "    width = 0.3\n",
    "    p1 = plt.bar(ind - width / 2.0, whole_mean[:,0], width=width, color='c', yerr=whole_std[:,0])\n",
    "    p2 = plt.bar(ind + width / 2.0, whole_mean[:,1], width=width, color='r', yerr=whole_std[:,1])\n",
    "    marker = [i.rstrip('/') for i in file_list]\n",
    "    plt.xticks(ind, marker)\n",
    "    plt.legend((p1[0], p2[0]),(\"Greedy (With Gradient Boosting)\", \"Random\"))\n",
    "    plt.ylabel(\"Num of Covered Areas\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def simulate_pipeline(change_k=False, change_p=False, change_cand=False):\n",
    "    if change_k:\n",
    "        changed_paras = range(5,40,5)\n",
    "        dir_name = 'change_k'\n",
    "        try:\n",
    "            os.listdir(dir_name)\n",
    "        except OSError:\n",
    "            os.mkdir(dir_name)\n",
    "    elif change_p:\n",
    "        changed_paras = [0.1, 0.01, 0.001]\n",
    "        dir_name = 'change_p'\n",
    "        try:\n",
    "            os.listdir(dir_name)\n",
    "        except OSError:\n",
    "            os.mkdir(dir_name)\n",
    "    elif change_cand:\n",
    "        changed_paras = range(100,600,100)\n",
    "        dir_name = 'change_cand'\n",
    "        try:\n",
    "            os.listdir(dir_name)\n",
    "        except OSError:\n",
    "            os.mkdir(dir_name)\n",
    "    else:\n",
    "        dir_name = 'single_test'\n",
    "        changed_paras = [0]\n",
    "\n",
    "    dataset_num = len(changed_paras)\n",
    "    iter_per_set = 10\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        plc_num = 1000\n",
    "        people = 600\n",
    "        candidate = 100\n",
    "        p = 0.001\n",
    "        q = 1-p\n",
    "        k_favor = 30\n",
    "\n",
    "        if change_k:\n",
    "            k_favor = changed_paras[i]\n",
    "        elif change_cand:\n",
    "            candidate = changed_paras[i]\n",
    "        elif change_p:\n",
    "            p = changed_paras[i]\n",
    "\n",
    "        new_simulation = Diff_Coverage(flip_p=p, flip_q=q, candidate_num=candidate,\n",
    "                                       plc_num=plc_num, people=people, k_favor=k_favor, max_iter=4000,\n",
    "                                       data_src=\"SG\", uniform=False,freeze=True,skew=True)\n",
    "        try:\n",
    "            os.remove(new_simulation.sample_dir)\n",
    "        except:\n",
    "            print \"No real sample detected!\"\n",
    "        try:\n",
    "            os.remove(new_simulation.transfer_dir)\n",
    "        except:\n",
    "            print \"No transferred data detected!\"\n",
    "        # new_simulation.real_sample(draw=True, skew=True, real=False)\n",
    "        # new_simulation.transfer_sample(draw=False)\n",
    "        # new_simulation.posterior_regression(draw=True)\n",
    "\n",
    "        for j in range(0, iter_per_set):\n",
    "            choice = new_simulation.train(use_grad=True)\n",
    "            result = new_simulation.validate(choice, times=1)\n",
    "            print result\n",
    "            with open(dir_name+ '/' + new_simulation.data_source + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\", 'w') as f:\n",
    "                f.write(json.dumps(result, indent=4))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    simulate_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.9899360678\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAD8CAYAAABgmUMCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN9IXDgeVfQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bbe3710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def easy_draw():\n",
    "    dataset_num = 3\n",
    "    method_num = 3\n",
    "    iter_per_set = 10\n",
    "    dir_name = \"change_cand\"\n",
    "    data_src = \"SG\"\n",
    "    color = ['c','r','b','g']\n",
    "    Results = np.zeros((dataset_num, method_num, iter_per_set))\n",
    "    Stats = np.zeros((dataset_num, method_num, 2))#0 for mean, 1 for std\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        for j in range(0, iter_per_set):\n",
    "            with open(dir_name+ '/' + data_src + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\",'r') as f:\n",
    "                results = json.loads(f.read())\n",
    "                for m in range(0, method_num):\n",
    "                    Results[i,m,j] = results[m][0]\n",
    "                    \n",
    "    for i in range(0,dataset_num):\n",
    "        for m in range(0, method_num):\n",
    "            Stats[i,m,0] = np.mean(Results[i,m,:])\n",
    "            Stats[i,m,1] = np.std(Results[i,m,:])\n",
    "\n",
    "    print Stats[0,0,1]\n",
    "    ind = np.arange(dataset_num)\n",
    "    width = 0.2\n",
    "    p_list = []\n",
    "    for m in range(0, method_num):\n",
    "        if method_num % 2 != 0:\n",
    "            bias = np.floor(method_num/2)\n",
    "        else:\n",
    "            bias = (method_num - 1.0)/2\n",
    "        p = plt.bar(ind + width*(m-bias), Stats[:,m,0], width=width, color=color[m], yerr=Stats[:,m,1])\n",
    "        p_list.append(p)\n",
    "            \n",
    "#     marker = ['p_0.1','p_0.01','p_0.001']\n",
    "#     marker = ['cand=30','cand=60','cand=90','cand=120']\n",
    "#     marker = ['k=10','k=20','k=30']\n",
    "    marker = ['cand=40','cand=42','cand=44']\n",
    "    plt.xticks(ind, marker)\n",
    "    plt.ylim(0,1000)\n",
    "    tup = tuple([p[0] for p in p_list])\n",
    "    plt.legend(tup,(\"Greedy\", \"Random\",\"Noisy\"))\n",
    "    plt.ylabel(\"Num of Covered Targets\")\n",
    "    plt.savefig(data_src+'_' + dir_name+'.png')\n",
    "    plt.show()\n",
    "\n",
    "easy_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n",
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCjE4JEmF/H/NdPB4LgYX2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bcb1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression over\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py27",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
