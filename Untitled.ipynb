{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load diff_coverage.py\n",
    "from __future__ import division\n",
    "from scipy.special import comb\n",
    "from scipy.stats import norm\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing as mp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import json\n",
    "from read_data import *\n",
    "import math\n",
    "\n",
    "def Gaussian_Approx(X, mu, std):\n",
    "    return -norm.cdf(x=(X-0.5-mu)/std) + norm.cdf(x=(X+0.5-mu)/std)\n",
    "\n",
    "class Diff_Coverage():\n",
    "\n",
    "    def __init__(self, uniform=True, f=0, flip_p=0.001, flip_q = 0.1, data_src=\"SG\",\n",
    "                 plc_num=200,people=500, candidate_num=50, k_favor=5,\n",
    "                 max_iter=200, stop_iter=200, freeze=False, skew=False):\n",
    "        # constants in rappor\n",
    "        self.f = f\n",
    "        self.p = flip_p\n",
    "        self.q = flip_q\n",
    "\n",
    "        # constants in datasets\n",
    "        self.plc_num = plc_num\n",
    "        self.k_favor = k_favor\n",
    "        self.people = people\n",
    "        self.candidate_num = candidate_num\n",
    "\n",
    "        # vector, different xi for different place if not uniform\n",
    "        self.uniform = uniform\n",
    "        self.xi = None\n",
    "\n",
    "        self.data_source = data_src\n",
    "\n",
    "\n",
    "        # constants in iteration\n",
    "        self.comb_mat = np.load('comb_mat.npy')\n",
    "        self.iter = max_iter\n",
    "        # used in greedy random select\n",
    "        self.stop_iter = stop_iter\n",
    "\n",
    "        # save to disk\n",
    "        self.sample_dir = \"sample.npy\"\n",
    "        self.transfer_dir = \"transfer.npy\"\n",
    "\n",
    "        # paras for regression: a for slope, b for intersect\n",
    "        # vector, different\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "\n",
    "        self._is_train = False\n",
    "        self.transferred_sample = None\n",
    "        self.true_sample = None\n",
    "\n",
    "        self.freeze = freeze\n",
    "        self.prob_mat = None\n",
    "        self.skew = skew\n",
    "\n",
    "    # make table for combination numbers\n",
    "    def make_table(self):\n",
    "        comb_mat = np.zeros((self.candidate_num + 1, self.candidate_num + 1))\n",
    "        for i in range(0, self.candidate_num + 1):\n",
    "            for j in range(0, i + 1):\n",
    "                comb_mat[i][j] = comb(i, j)\n",
    "        np.save('comb_mat', comb_mat)\n",
    "\n",
    "    def update_paras(self):\n",
    "        self.a = np.zeros(self.plc_num)\n",
    "        self.b = np.zeros(self.plc_num)\n",
    "        # default is a uniform prior\n",
    "        self.xi = np.ones(self.plc_num) * self.k_favor / (self.plc_num + 0.0)\n",
    "\n",
    "    def posterior(self, X, plc_id):\n",
    "        N = self.candidate_num\n",
    "        upper_bound = 500\n",
    "        p_0 = self.p - 0.5 * self.f * (self.p - self.q)\n",
    "        p_1 = self.q + 0.5 * self.f * (self.p - self.q)\n",
    "\n",
    "        xi = self.xi[plc_id]\n",
    "        sum_ = 0\n",
    "        numerator = p_0 ** X * (1 - p_0) ** (N - X)\n",
    "        # use Gaussian to approximate Binomial when N is large\n",
    "        if N > upper_bound:\n",
    "            mu = N * p_0\n",
    "            std = math.sqrt(N * p_0 * (1 - p_0))\n",
    "            numerator *= Gaussian_Approx(X,mu,std)\n",
    "        else:\n",
    "            numerator *= self.comb_mat[N, X] * (1 - xi) ** N\n",
    "        for i in range(0, N + 1):\n",
    "            if N > upper_bound:\n",
    "                mu = N * xi\n",
    "                std = math.sqrt(N * xi * (1 - xi))\n",
    "                outer = Gaussian_Approx(i, mu, std)\n",
    "            else:\n",
    "                outer = self.comb_mat[N, i] * xi ** i * (1 - xi) ** (N - i)\n",
    "            if outer == 0.0:\n",
    "                continue\n",
    "            inner = 0\n",
    "            for m in range(max([0, X + i - N]), min([i, X]) + 1):\n",
    "                # sum_ += self.comb_mat[N, i] * self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #     self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m) * \\\n",
    "                #     xi ** i * (1 - xi) ** (N - i)\n",
    "                if i <= upper_bound:\n",
    "                    first_part = self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m)\n",
    "                else:\n",
    "                    mu = i * (1 - p_1)\n",
    "                    std = math.sqrt(i * p_1 * (1 - p_1))\n",
    "                    first_part = Gaussian_Approx(m, mu, std)\n",
    "                if N - i > upper_bound:\n",
    "                    mu = (N-i) * p_0\n",
    "                    std = math.sqrt((N-i) * p_0 * (1 - p_0))\n",
    "                    second_part = Gaussian_Approx(X-m,mu,std)\n",
    "                else:\n",
    "                    second_part = self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                # inner += self.comb_mat[i, m] * p_1 ** m * (1 - p_1) ** (i - m) * \\\n",
    "                #         self.comb_mat[N - i, X - m] * p_0 ** (X - m) * (1 - p_0) ** (N - i - X + m)\n",
    "                inner += first_part * second_part\n",
    "            sum_ += outer * inner\n",
    "        ratio = numerator / sum_\n",
    "        return ratio\n",
    "\n",
    "    # probability distribution for artificial dataset\n",
    "    def prob_dist(self):\n",
    "        plcs = []\n",
    "        y = np.ones(self.plc_num) * 1.0 / self.plc_num\n",
    "\n",
    "        prob_mat = np.zeros((self.plc_num, 3))\n",
    "        prob_mat[:, 0] = np.arange(0, self.plc_num)\n",
    "        prob_mat[:, 1] = y\n",
    "        prob_mat[:, 2] = prob_mat[:, 1].cumsum()\n",
    "\n",
    "        for plc in range(0, self.k_favor):\n",
    "            in_bit = 1\n",
    "            while in_bit == 1:\n",
    "                r = np.random.rand()\n",
    "                plc_num = prob_mat.shape[0]\n",
    "                index = -1\n",
    "                for i in range(0, plc_num - 1):\n",
    "                    if i == 0:\n",
    "                        if prob_mat[i, 2] >= r:\n",
    "                            index = prob_mat[i, 0]\n",
    "                            break\n",
    "                    if prob_mat[i, 2] < r and prob_mat[i + 1, 2] >= r:\n",
    "                        index = prob_mat[i + 1, 0]\n",
    "                        break\n",
    "                if index not in plcs:\n",
    "                    plcs.append(int(index))\n",
    "                    in_bit = 0\n",
    "                # else:\n",
    "                #     print \"Same found\"\n",
    "        return plcs\n",
    "\n",
    "    # read real dataset from different resources\n",
    "    def real_sample(self, draw=False):\n",
    "        if self.data_source == \"MCS\" or \"SG\" or \"CG\":\n",
    "            if self.data_source == \"MCS\":\n",
    "                rd = MCS(k_favor=self.k_favor, people=self.people)\n",
    "                sample = rd.make_grid()\n",
    "                self.plc_num = rd.plc_num\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            elif self.data_source == \"SG\":\n",
    "                rd = SG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "                # if not self.uniform:\n",
    "                #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "            else:\n",
    "                rd = CG(k_favor=self.k_favor, people=self.people, max_id=self.plc_num)\n",
    "                sample = rd.make_grid()\n",
    "            if self.skew:\n",
    "                print \"Not real dataset, add artificial groups\"\n",
    "                num = int(self.plc_num/self.k_favor)\n",
    "                full_group = np.zeros((num, self.plc_num))\n",
    "                for row in range(0,num):\n",
    "                    full_group[row, range((row-1)*self.k_favor, row*self.k_favor)] = 1\n",
    "                sample = np.vstack((sample, full_group))\n",
    "                self.people = sample.shape[0]\n",
    "            np.save(self.sample_dir, sample)\n",
    "            print \"Total recruit: \", self.people, \" from places: \", self.plc_num\n",
    "            self.update_paras()\n",
    "            # if not self.uniform:\n",
    "            #     self.xi = np.sum(sample, axis=0) / (0.0 + self.people)\n",
    "        else:\n",
    "            sample = np.zeros((self.people, self.plc_num))\n",
    "\n",
    "            # skew: add groups of people covering the whole area\n",
    "            if self.skew:\n",
    "                partition = [0.2, 0.3, 0.5]\n",
    "                self.k_favor = self.plc_num / self.candidate_num\n",
    "                all_plc = list(np.random.permutation(self.plc_num))\n",
    "                for i in range(0, int(self.people * partition[0])):\n",
    "                    mod = i % self.candidate_num\n",
    "                    sample[i,all_plc[mod * self.k_favor: (mod + 1) * self.k_favor]] = 1\n",
    "                same_k_favor = random.sample(range(self.plc_num), self.k_favor * 5)\n",
    "                for i in range(int(self.people * partition[0]), int(self.people * partition[1])):\n",
    "                    small_partition = random.sample(same_k_favor, self.k_favor)\n",
    "                    sample[i, small_partition] = 1\n",
    "                for i in range(int(self.people * partition[1]), int(self.people * partition[2])):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = 1\n",
    "            else:\n",
    "                for i in range(0, self.people):\n",
    "                    index = self.prob_dist()\n",
    "                    sample[i, index] = np.ones(self.k_favor)\n",
    "            self.update_paras()\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        return sample\n",
    "\n",
    "    def flip(self, bit):\n",
    "        r = np.random.rand()\n",
    "        if r < 0.5 * self.f:\n",
    "            B_prime = 1\n",
    "        elif r < self.f:\n",
    "            B_prime = 0\n",
    "        else:\n",
    "            B_prime = bit\n",
    "        r = np.random.rand()\n",
    "        if B_prime == 1:\n",
    "            if r < self.q:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        else:\n",
    "            if r < self.p:\n",
    "                B = 1\n",
    "            else:\n",
    "                B = 0\n",
    "        return B\n",
    "\n",
    "    # create \"fake\" dataset after random response\n",
    "    def transfer_sample(self, draw=False):\n",
    "        sample = self.real_sample()\n",
    "        self.true_sample = sample.copy()\n",
    "        if not self.uniform:\n",
    "            self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "            self.xi[self.xi == 0] = 1.0 / self.people\n",
    "        for i in range(0, self.people):\n",
    "            r = sample[i, :]\n",
    "            r = np.array(map(lambda bit: self.flip(bit), r))[np.newaxis, :]\n",
    "            sample[i, :] = r\n",
    "\n",
    "        row_sum = np.sum(sample, axis=0)\n",
    "        if draw:\n",
    "            plt.plot(np.arange(0, self.plc_num), row_sum)\n",
    "            plt.show()\n",
    "        self.transferred_sample = sample\n",
    "        np.save(self.transfer_dir, sample)\n",
    "\n",
    "    # sum posterior in terms of locations\n",
    "    def sum_posterior(self, sum_row):\n",
    "        if self.uniform and self.prob_mat is not None:\n",
    "            posterior_per_loc = [self.prob_mat[int(x)] for x in sum_row]\n",
    "        else:\n",
    "            posterior_per_loc = [self.posterior(int(x), id) for id, x in enumerate(sum_row)]\n",
    "        total_sum = np.sum(np.array(posterior_per_loc))\n",
    "        return total_sum\n",
    "\n",
    "    def train(self, use_grad=False, fake=False):\n",
    "        if not os.path.exists(self.sample_dir) or \\\n",
    "                not os.path.exists(self.transfer_dir) or not self.freeze:\n",
    "            self.transfer_sample()\n",
    "            sample = self.transferred_sample\n",
    "        else:\n",
    "            self.transferred_sample = np.load(self.transfer_dir)\n",
    "            self.true_sample = np.load(self.sample_dir)\n",
    "            sample = self.transferred_sample\n",
    "            if not self.uniform and not fake:\n",
    "                self.update_paras()\n",
    "                self.xi = np.sum(self.true_sample, axis=0) / self.people\n",
    "                # self.xi[np.where(self.xi)==0] = 1 / self.people\n",
    "\n",
    "        # first fit the linear regression coefficients\n",
    "        self.posterior_regression(draw=False)\n",
    "\n",
    "        # choice means those ids which are in the candidate set\n",
    "        choice = random.sample(range(self.people), self.candidate_num)\n",
    "        not_choice = list(set(range(self.people)).difference(set(choice)))\n",
    "\n",
    "        # party means the bit arrays for those candidates\n",
    "        party = sample[choice, :]\n",
    "        unused = sample[not_choice, :]\n",
    "\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print \"Start Training: Original Loss - \" + str(total_sum)\n",
    "\n",
    "        same_count = 0\n",
    "        past_set = set()\n",
    "        for i in range(0, self.iter):\n",
    "\n",
    "            if use_grad:\n",
    "                diff, id_of_r, id_of_r_ = self.grad_boosting(\n",
    "                    sum_row, party, unused)\n",
    "                r = choice[id_of_r]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "                if diff > 0:\n",
    "                    if {r,r_} == past_set:\n",
    "                        same_count += 1\n",
    "                    else:\n",
    "                        past_set = {r, r_}\n",
    "                        same_count = 0\n",
    "                    if same_count >= 3:\n",
    "                        print \"Stop iteration at: \", i\n",
    "                        break\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "\n",
    "                    # print 'Loss reduce by: ', str(diff), 'r and r_ is: ', r, r_\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                id_of_r = np.random.randint(self.candidate_num)\n",
    "                r = choice[id_of_r]\n",
    "                id_of_r_ = random.sample(range(len(unused)), 1)[0]\n",
    "                r_ = not_choice[id_of_r_]\n",
    "\n",
    "                sum_row_new = sum_row - party[id_of_r, :] + sample[r_, :]\n",
    "                total_sum_new = self.sum_posterior(sum_row_new)\n",
    "\n",
    "                if total_sum_new < total_sum:\n",
    "                    total_sum = total_sum_new\n",
    "                    choice[id_of_r] = r_\n",
    "                    not_choice[id_of_r_] = r\n",
    "                    party = sample[choice, :]\n",
    "                    unused = sample[not_choice, :]\n",
    "\n",
    "                    sum_row = np.sum(party, axis=0)\n",
    "                    same_count = 0\n",
    "                else:\n",
    "                    same_count += 1\n",
    "\n",
    "                # print str(total_sum), same_count\n",
    "\n",
    "                if same_count > 1 and use_grad:\n",
    "                    break\n",
    "                elif same_count > self.stop_iter:\n",
    "                    break\n",
    "        total_sum = self.sum_posterior(sum_row)\n",
    "        print 'Finish:', total_sum, same_count\n",
    "        self._is_train = True\n",
    "        return choice\n",
    "\n",
    "    # compare with two baseline: use \"fake data\" directly and greedy random search\n",
    "    def validate(self, choice, times=10):\n",
    "        if not self._is_train:\n",
    "            exit(1)\n",
    "        # fail = 0\n",
    "        # abnormal = 0\n",
    "        true_sample = self.true_sample\n",
    "        fake_sample = self.transferred_sample\n",
    "\n",
    "        result = []\n",
    "        true_opt_party = true_sample[choice, :]\n",
    "        fake_opt_party = fake_sample[choice, :]\n",
    "        sum_row = np.sum(fake_opt_party, axis=0)\n",
    "        total_sum_opt = self.sum_posterior(sum_row)\n",
    "        coverage_opt = np.sum(np.sum(true_opt_party, axis=0) > 0)\n",
    "        result.append((coverage_opt, total_sum_opt))\n",
    "        for i in range(times):\n",
    "            # totally random pick:\n",
    "            print \"Random Pick\"\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            result.append((coverage_rand, total_sum_rand))\n",
    "\n",
    "            # greedy search\n",
    "            # print \"Rand_Greedy\"\n",
    "            # rand_greedy_choice = self.train(use_grad=False)\n",
    "            # rand_greedy_party = true_sample[rand_greedy_choice,:]\n",
    "            # rand_greedy_fake_party = fake_sample[rand_greedy_choice,:]\n",
    "            #\n",
    "            # sum_row = np.sum(rand_greedy_fake_party, axis=0)\n",
    "            # total_sum_rand_greedy = self.sum_posterior(sum_row)\n",
    "            # coverage_rand_greedy = np.sum(np.sum(rand_greedy_party, axis=0) > 0)\n",
    "            # result.append((coverage_rand_greedy, total_sum_rand_greedy))\n",
    "\n",
    "            # take fake as real\n",
    "\n",
    "            print \"Noisy\"\n",
    "            raw_p, raw_q = (self.p, self.q)\n",
    "            self.p,self.q = (1e-10, 1-1e-10)\n",
    "            if not self.uniform:\n",
    "                raw_xi = self.xi\n",
    "                self.xi = np.sum(fake_sample, axis=0) / self.people\n",
    "            fake_choice = self.train(use_grad=True,fake=True)\n",
    "            # print fake_choice\n",
    "            fake_party = true_sample[fake_choice,:]\n",
    "            fake_fake_party = fake_sample[fake_choice,:]\n",
    "\n",
    "            sum_row = np.sum(fake_fake_party, axis=0)\n",
    "            total_sum_fake = self.sum_posterior(sum_row)\n",
    "            coverage_fake = np.sum(np.sum(fake_party, axis=0) > 0)\n",
    "            result.append((coverage_fake, total_sum_fake))\n",
    "            self.p, self.q = (raw_p, raw_q)\n",
    "            if not self.uniform:\n",
    "                self.xi = raw_xi\n",
    "\n",
    "        # first element is opt, second is random-greedy, third is fake-data\n",
    "        return result\n",
    "\n",
    "    # plot random select results to find relationship between loss function and target\n",
    "    def random_select(self, random_choice=50, file_name='p_0.01'):\n",
    "        true_sample = np.load(self.sample_dir)\n",
    "        fake_sample = np.load(self.transfer_dir)\n",
    "        rand_prob_ = np.zeros(random_choice)\n",
    "        rand_region_ = np.zeros(random_choice)\n",
    "\n",
    "        for i in range(0, random_choice):\n",
    "            self.iter = np.random.randint(1000, 1500)\n",
    "            self.stop_iter = np.random.randint(50, 100)\n",
    "\n",
    "            rand_choice = random.sample(range(self.people), self.candidate_num)\n",
    "            # choice = self.train()\n",
    "            true_rand_party = true_sample[rand_choice, :]\n",
    "            fake_rand_party = fake_sample[rand_choice, :]\n",
    "\n",
    "            # print out the posterior\n",
    "            sum_row = np.sum(fake_rand_party, axis=0)\n",
    "            total_sum_rand = self.sum_posterior(sum_row)\n",
    "            rand_prob_[i] = total_sum_rand\n",
    "            coverage_rand = np.sum(np.sum(true_rand_party, axis=0) > 0)\n",
    "            rand_region_[i] = coverage_rand\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print i\n",
    "\n",
    "        # plt.scatter(real_prob_, real_region,c='r')\n",
    "        plt.scatter(rand_prob_, rand_region_, c='b')\n",
    "        plt.xlabel(\"Sum of posterior over location\")\n",
    "        plt.ylabel(\"Coverage\")\n",
    "        title = \"transfer_prob_=\" + str(self.p) + \", candidate_num=\" + str(\n",
    "            self.candidate_num) + \", k_favor=\" + str(self.k_favor)\n",
    "        plt.title(title)\n",
    "        plt.savefig('img/' + file_name + '.png')\n",
    "        print self.p, self.k_favor, self.candidate_num\n",
    "\n",
    "    def posterior_regression(self, draw=False):\n",
    "        # now coefficients should be a matrix, each place has its own slope and intercept\n",
    "\n",
    "        # only take several points to fit the linear regression,\n",
    "        # avoiding high cost of iteration\n",
    "\n",
    "        # x_max = self.candidate_num + 1\n",
    "        x_max = 10\n",
    "        def find_coeffs(plc_id):\n",
    "            X_range = np.arange(0, x_max)\n",
    "            stop_count = self.candidate_num + 1\n",
    "            ans = np.zeros(X_range.shape)\n",
    "            for X in X_range:\n",
    "                ans[X] = np.log(self.posterior(X=X, plc_id=plc_id))\n",
    "                if ans[X] == -np.inf or ans[X] == np.nan:\n",
    "                    ans[X] = 0\n",
    "                    stop_count = X\n",
    "                    break\n",
    "            X_range = X_range[0:stop_count, np.newaxis]\n",
    "            ans = ans[0:stop_count, np.newaxis]\n",
    "            if (draw):\n",
    "                plt.scatter(X_range, ans)\n",
    "                plt.show()\n",
    "            lr = LinearRegression()\n",
    "            lr.fit(X_range, ans)\n",
    "            return lr.coef_, lr.intercept_\n",
    "\n",
    "        if self.uniform:\n",
    "            # print \"Make posterior tables!\"\n",
    "            self.prob_mat = np.array([self.posterior(x,1) for x in range(self.candidate_num + 1)])\n",
    "            self.prob_mat[np.isnan(self.prob_mat)] = 0\n",
    "            coeffs = find_coeffs(plc_id=0)\n",
    "            self.a = np.ones(self.a.shape) * coeffs[0]\n",
    "            self.b = np.ones(self.b.shape) * coeffs[1]\n",
    "        else:\n",
    "            # self.prob_mat = np.zeros((self.plc_num, self.candidate_num + 1))\n",
    "            # plc with same prior just need one time calculation\n",
    "            prior_dict = {}\n",
    "\n",
    "            for plc_id in range(self.plc_num):\n",
    "                xi_id = int(np.around(self.xi[plc_id] * self.people))\n",
    "                if prior_dict.has_key(xi_id):\n",
    "                    # self.prob_mat[plc_id,:] = self.prob_mat[prior_dict[xi_id],:]\n",
    "                    self.a[plc_id] = self.a[prior_dict[xi_id]]\n",
    "                    self.b[plc_id] = self.b[prior_dict[xi_id]]\n",
    "                else:\n",
    "                    # for x in range(0,self.candidate_num+1):\n",
    "                    #     self.prob_mat[plc_id, x] = self.posterior(X=x, plc_id=plc_id)\n",
    "                    prior_dict[xi_id] = plc_id\n",
    "                    coeffs = find_coeffs(plc_id)\n",
    "                    self.a[plc_id] = coeffs[0]\n",
    "                    self.b[plc_id] = coeffs[1]\n",
    "                # if xi_id == 0:\n",
    "                #     print self.a[plc_id], self.b[plc_id]\n",
    "            print \"Total different prior: \", len(prior_dict)\n",
    "\n",
    "        print \"Linear Regression over\"\n",
    "\n",
    "    def grad_boosting(self, sum_row, party, unused):\n",
    "\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_sub = np.dot(party, grad_)\n",
    "        id_of_r = np.argmin(candidate_sub)\n",
    "\n",
    "        # re-calculate grad_ using party after removing a candidate\n",
    "        party[id_of_r,:] = 0\n",
    "        sum_row = np.sum(party, axis=0)\n",
    "        grad_ = abs(self.a * np.exp(self.b) * np.exp(self.a * sum_row)).T\n",
    "        candidate_add = np.dot(unused, grad_)\n",
    "\n",
    "        # return their sequential id, not original id in sample\n",
    "        id_of_r_ = np.argmax(candidate_add)\n",
    "\n",
    "        diff = np.max(candidate_add) - np.min(candidate_sub)\n",
    "        return diff, int(id_of_r), int(id_of_r_)\n",
    "\n",
    "def simulate_pipeline(change_k=False, change_p=False, change_cand=False, add_skew=False):\n",
    "    if change_k:\n",
    "        changed_paras = range(10,40,10)\n",
    "        dir_name = 'change_k'\n",
    "    elif change_p:\n",
    "        changed_paras = [0.1, 0.01, 0.001]\n",
    "        dir_name = 'change_p'\n",
    "    elif change_cand:\n",
    "        changed_paras = range(40,45,2)\n",
    "        dir_name = 'change_cand'\n",
    "    elif add_skew:\n",
    "        changed_paras = [True, False]\n",
    "        dir_name = 'add_skew'\n",
    "    else:\n",
    "        dir_name = 'test'\n",
    "        changed_paras = [0]\n",
    "\n",
    "    try:\n",
    "        os.listdir(dir_name)\n",
    "    except OSError:\n",
    "        os.mkdir(dir_name)\n",
    "\n",
    "    dataset_num = len(changed_paras)\n",
    "    iter_per_set = 10\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        print \"Start a new set!\"\n",
    "        plc_num = 1000\n",
    "        people = 600\n",
    "        candidate = 40\n",
    "        p = 1e-3\n",
    "        q = 1-p\n",
    "        k_favor = 25\n",
    "        skew = True\n",
    "\n",
    "        if change_k:\n",
    "            k_favor = changed_paras[i]\n",
    "        elif change_cand:\n",
    "            candidate = changed_paras[i]\n",
    "        elif change_p:\n",
    "            p = changed_paras[i]\n",
    "        elif add_skew:\n",
    "            skew = changed_paras[i]\n",
    "\n",
    "        new_simulation = Diff_Coverage(flip_p=p, flip_q=q, candidate_num=candidate,\n",
    "                                       plc_num=plc_num, people=people, k_favor=k_favor, max_iter=4000,\n",
    "                                       data_src=\"SG\", uniform=False,freeze=True,skew=skew)\n",
    "        try:\n",
    "            os.remove(new_simulation.sample_dir)\n",
    "        except:\n",
    "            print \"No real sample detected!\"\n",
    "        try:\n",
    "            os.remove(new_simulation.transfer_dir)\n",
    "        except:\n",
    "            print \"No transferred data detected!\"\n",
    "        # new_simulation.real_sample(draw=True, skew=True, real=False)\n",
    "        # new_simulation.transfer_sample(draw=False)\n",
    "        # new_simulation.posterior_regression(draw=True)\n",
    "\n",
    "        for j in range(0, iter_per_set):\n",
    "            choice = new_simulation.train(use_grad=True)\n",
    "            result = new_simulation.validate(choice, times=1)\n",
    "            print result\n",
    "            with open(dir_name+ '/' + new_simulation.data_source + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\", 'w') as f:\n",
    "                f.write(json.dumps(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'simu.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-497897d12fbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0measy_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-497897d12fbe>\u001b[0m in \u001b[0;36measy_draw\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0measy_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"simu.json\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmethod_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'method_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'simu.json'"
     ]
    }
   ],
   "source": [
    "def easy_draw():\n",
    "    with open(\"simu.json\",'r') as f:\n",
    "        d = json.loads(f.read())\n",
    "    dataset_num = d['dataset_num']\n",
    "    method_num = d['method_num']\n",
    "    iter_per_set = d['iter_per_set']\n",
    "    dir_name = d['dir_name']\n",
    "    data_src = d['data_src']\n",
    "    marker = d['change_paras']\n",
    "    title = d['title']\n",
    "    \n",
    "#     marker = ['cand=30','cand=60','cand=90','cand=120']\n",
    "#     marker = ['k=10','k=20','k=30']\n",
    "#     marker = ['cand=40','cand=42','cand=44']\n",
    "\n",
    "\n",
    "    color = ['c','b','r','y','m','k']\n",
    "    Results = np.zeros((dataset_num, method_num, iter_per_set))\n",
    "    Stats = np.zeros((dataset_num, method_num, 2))#0 for mean, 1 for std\n",
    "\n",
    "    for i in range(0, dataset_num):\n",
    "        for j in range(0, iter_per_set):\n",
    "            with open(dir_name+ '/' + data_src + '_' +\n",
    "                      str(i) + '_' + str(j) + \".json\",'r') as f:\n",
    "                results = json.loads(f.read())\n",
    "                for m in range(0, method_num):\n",
    "                    Results[i,m,j] = results[m][0]\n",
    "                    \n",
    "    for i in range(0,dataset_num):\n",
    "        for m in range(0, method_num):\n",
    "            Stats[i,m,0] = np.mean(Results[i,m,:])\n",
    "            Stats[i,m,1] = np.std(Results[i,m,:])\n",
    "\n",
    "    print Stats[0,0,1]\n",
    "    ind = np.arange(dataset_num)\n",
    "    width = 0.2\n",
    "    p_list = []\n",
    "    for m in range(0, method_num):\n",
    "        if method_num % 2 != 0:\n",
    "            bias = np.floor(method_num/2)\n",
    "        else:\n",
    "            bias = (method_num - 1.0)/2\n",
    "        p = plt.bar(ind + width*(m-bias), Stats[:,m,0], width=width, color=color[m], yerr=Stats[:,m,1])\n",
    "        p_list.append(p)\n",
    "            \n",
    "    plt.xticks(ind, marker)\n",
    "    plt.ylim(500,650)\n",
    "    tup = tuple([p[0] for p in p_list])\n",
    "    plt.legend(tup,(\"Greedy\", \"Noisy\",\"Random\"))\n",
    "    plt.ylabel(\"Num of Covered Targets\")\n",
    "    plt.savefig(d['pic_name'])\n",
    "    plt.show()\n",
    "\n",
    "easy_draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n",
      "Present recruit:  1146\n",
      "Total recruit:  500  from places:  200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCjE4JEmF/H/NdPB4LgYX2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110bcb1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression over\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_py27",
   "language": "python",
   "name": "anaconda_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
